# This is a pipeline template for metabarcoding, going from demultiplexed reads to OTUs and read abundance data.
# See the Wiki and the LearnMetabarcoding site for more details on these steps and the concepts behind them - this document should act as a reference only.


##                  ##
## READ PREPARATION ##
##                  ##

### Run trimming on paired files in a loop. Assumes _R[12].fastq. ###

    # Get a list of unique samples. 

samples=$(for f in 0_demux/*.fastq; do s1=${f##*/}; echo ${s1%_*}; done | sort | uniq)
echo $samples 
    # If this looks right, proceed, if not you may need to tweak paths/names in previous command

mkdir 1_trimmed
rm -f 1_trim_report.txt

    # Do trimming, make sure that primers are right: -g is forward, -G is reverse primer. -j is threads, don't increase!
    # Update: because of ambiguities, the beginning of the forward primer can be matched by the end, i.e CCN = NCG, CCNG = CNCG. So for this primer we set a larger-than-default minimum overlap.
for s in $samples
do
   cutadapt -j 20 -g "CCNGAYATRGCNTTYCCNCG;min_overlap=5" -G TANACYTCNGGRTGNCCRAARAAYCA -o 1_trimmed/${s}_R1.fastq -p 1_trimmed/${s}_R2.fastq --minimum-length 50 --discard-untrimmed 0_demux/${s}_R1.fastq 0_demux/${s}_R2.fastq 2>&1 | tee -a 1_trim_report.txt
done

    # --minimum-length 50 discards reads shorter than 50bp, as these are undoubtedly errors and it's a waste of later computation time keeping them

    # To get the counts of reads in each file in a tsv
rm -f 1_trim_outcounts.tsv
for s in $samples
do
   for d in R1 R2
   do
      c=$(grep -c '^+$' 1_trimmed/${s}_$d.fastq)
      echo -e "$s\t$d\t$c" >> 1_trim_outcounts.tsv
   done
done


### Run merging on paired files in a loop. ###

  # Get a list of unique samples like above
samples=$(for f in 1_trimmed/*.fastq; do s1=${f##*/}; echo ${s1%_*}; done | sort | uniq)
echo $samples

mkdir 2_merged
rm -f 2_merge_report.txt

  # Do merging
for s in $samples
do
   pear -f 1_trimmed/${s}_R1.fastq -r 1_trimmed/${s}_R2.fastq -o 2_merged/$s -q 26 -v 60 -j 20 2>&1 | tee -a 2_merge_report.txt
done

  # Clean up uneeded files
cd 2_merged/
rm *discarded* *unassembled* 
rename -e "s/assembled\.//" *
cd ../
    
    # To get the counts of reads in each file in a tsv
rm -f 2_merge_outcounts.tsv
for s in $samples
do
   c=$(grep -c '^+$' 2_merged/$s.fastq)
   echo -e "$s\t$c" >> 2_merge_outcounts.tsv
done


### Concatenating ###

   # Get a list of unique samples similar to above
samples=$(for f in 2_merged/*.fastq; do s1=${f##*/}; echo ${s1%.*}; done | sort | uniq)
echo $samples

  # Find the unique sequence headers in your files
    # The -c option determines which characters are reported
head -q -n 1 0_demux/*.fastq | cut -c 1-10 | sort | uniq

    # MAKE A NOTE OF THESE!

  # Do concatenation
  # Important note: change @HEAD in the sed command for the start of the headers of your sequences (including the @)
  # Try and use >=~4 characters to avoid quality lines starting with @ (rare, but it happens).
rm -f 3_mbc_concat.fastq
for s in $samples
do
   sed -e "s/\(^@HEAD.*\) .*$/\1;sample=$s;/" 2_merged/$s.fastq >> 3_mbc_concat.fastq
done

  # If you have multiple headers, use this command instead, using the @HEADN[^ ]* as many times as needed separated by |
rm -f 3_mbc_concat.fastq
for s in $samples
do
   sed -E "s/^(@HEAD1[^ ]*|@HEAD2[^ ]*|@HEAD3[^ ]*).*$/\1;sample=$s;/" 2_merged/$s.fastq >> 3_mbc_concat.fastq
done

##                                  ##
## MAPPING READS TO A REFERENCE SET ##
##                                  ##

# Note - this step is only applicable to certain pipelines, where you are only interested in identifying the indicence of
# a specific set of taxa that are comprehensively represented in a reference set. For example if you're trying to see if you
# have reads of a specific species or set of species for which you already have barcodes. This is totally irrelevant if
# you're doing metabarcoding where you both expect and are interested in OTUs of unknown identity

vsearch --fastx_filter 3_mbc_concat.fastq --fastq_maxee 1 --fastaout 3_mbc_concat.fasta
vsearch --usearch_global 3_mbc_concat.fasta -db references.fasta -id 0.97 -otutabout reads_map_reference.tsv

# If you have done this step, you are now done and you would analyse the output tsv. Everything else would be irrelevant.

##                     ##
## FILTERING AMPLICONS ##
##                     ##

### Quality filtering ###

vsearch --fastx_filter 3_mbc_concat.fastq --fastq_maxee 1 --fastaout 3_mbc_concat.fasta

    # Remember, the file 3_mbc_concat.fasta is important and will be used later!

### Dereplication ###

vsearch --derep_fulllength 3_mbc_concat.fasta --output 4_mbc_derep.fasta --sizeout --relabel uniq

### Denoising ###

vsearch --cluster_unoise 4_mbc_derep.fasta --minsize 4 --unoise_alpha 2 --centroids 5_mbc_denoise.fasta

### Length filtering ###

vsearch --fastx_filter 5_mbc_denoise.fasta --fastq_minlen 418 --fastq_maxlen 418 -fastaout 6_mbc_indelfil.fasta

### Translation filtering ##
  
  # Run the filtering (5 is the translation table)
  # If it can't find filtertranslate, run `python3 -m pip install metaMATE`
filtertranslate -i 6_mbc_indelfil.fasta -t 5 -o 7_mbc_transpass.fasta

### Chimera filtering ###

vsearch --uchime3_denovo 7_mbc_transpass.fasta --nonchimeras 8_mbc_final.fasta

  # 8_mbc_final.fasta contains your ASVs

##                       ##
## MAPPING READS TO ASVS ##
##                       ##

# Whether you are working on ASVs or OTUs determines whether you want to do this step
vsearch --search_exact 3_mbc_concat.fasta -db 8_mbc_final.fasta -otutabout reads_asv_map.tsv

##                  ##
## OTU DELIMITATION ##
##                  ##

### Standard delimitation method that we usually do ###

vsearch --cluster_size 8_mbc_final.fasta --sizein --relabel otu --id 0.97 --centroids otus.fasta

# If you want to record which ASVs form which OTUs:
vsearch --cluster_size 8_mbc_final.fasta --sizein --relabel otu --id 0.97 --centroids otus.fasta -uc asv_otu.uc

# The following line should convert the uc into an easy-to-read table but no guarantees that it's correct!
paste <(grep -v "^C" asv_otu.uc | cut -f9) <(grep -v "^C" asv_otu.uc | cut -f2 | while read l; do echo "otu$(($l + 1))"; done) > asv_otu.tsv


### SWARM alternative ###

swarm -w otus.fasta -d 1 -z 8_mbc_final.fasta

  # Don't bother with CROP

##                       ##
## MAPPING READS TO OTUS ##
##                       ##

  # Standard approach generating standard table
vsearch --usearch_global 3_mbc_concat.fasta -db otus.fasta -id 0.97 -otutabout reads_otu_map.tsv

  # Alternative approach generating detailed searching output
vsearch --usearch_global 3_mbc_concat.fasta -db otus.fasta -id 0.97 -uc reads_otu_search.uc

##                                 ##
## TAXONOMY CLASSIFICATION OF OTUS ##
##                                 ##

# Remember, classification is the sorting of your OTUs into taxonomic groups, generally to filter out OTUs that are not of
# interest and/or to understand broad structures of your data. BLASTN+MEGAN or SINTAX are designed to approximately assign
# taxonomy based on broad databases, and work on the principle that it is unlikely you will have exact matches at lower
# taxonomic levels, but that taxonomy can be estimated by the consideration of the more distant matches that are found. You 
# should never expect to be getting species-level matches from these methods, and you should be very wary of using low-level
# matches from these methods as identifications.

  # Using BLASTn vs Genbank NT - the output from this would then be loaded into MEGAN
blastn -db <path to nt> -query otus.fasta -outfmt 5 -out otus_nt_blast.xml -num_threads 15 -evalue 0.001
  # Go and load this xml into MEGAN and output the results as readName_to_taxonId 
  # Then run this to get the identifications in a nice format
get_NCBI_taxonomy.py -i otus_nt_blast_MEGAN.txt -l </path/to/>voglerlab/reference/NCBI_taxid2taxonomy.json -o <outname.txt>

  # Using BLASTn vs your own reference set
blastn -query otus.fasta -subject references.fasta -outfmt 6 -out otus_reference_blast.txt -num_threads 1 -evalue 0.001 -perc_identity 97

  # Using SINTAX vs the MIDORI dataset.
# Go to http://reference-midori.info/download/Databases/ and check the most recent version to update the following commands
        # Make the database
wget http://reference-midori.info/download/Databases/GenBank249/SINTAX/uniq/MIDORI_UNIQ_NUC_GB249_CO1_SINTAX.fasta.gz
gzip -d MIDORI_UNIQ_NUC_GB249_CO1_SINTAX.fasta.gz
vsearch --makeudb_usearch MIDORI_UNIQ_NUC_GB249_CO1_SINTAX.fasta --output MIDORI_UNIQ_NUC_GB249_CO1_SINTAX.udb
vsearch -sintax otus.fasta -db MIDORI_UNIQ_NUC_GB249_CO1_SINTAX.udb -tabbedout otus_midori_sintax.tsv -strand both -sintax_cutoff 1

  # Filtering sequences based on a SINTAX output
filter_fasta_by_sintax.py -f otus.fasta -s otus_midori_sintax.tsv -t o:Coleoptera > otus_filtered.fasta

##                        ##
## IDENTIFICATION OF OTUS ##
##                        ##

# Remember, identification is the process of assigning a low-level taxonomic name to an OTU, i.e. a genus or species.
# Generally, BLAST+MEGAN or SINTAX are not designed for this. Realistically, a identification can only be made through a very
# high identity match against a reference in which you have confidence. These might be your own references that you've
# generated, or some sequences you've selectively compiled and curated from GenBank or BOLD for this purpose. Assigning an
# identification is a strong statement of certainty and you must have the confidence behind this. The below commands 
# represent a starting point for how this should be done, but this should be very much your own process and require your 
# own careful consideration.

    # Identifying your OTUs against your reference set - you might want to increase the id value
vsearch --usearch_global otus.fasta -db references.fasta -id 0.98 -uc otus_search.uc

# If you want to rename your OTUs according to your reference set, you can use my script. This renames OTUs in the otu fasta and the reads mapping table to the name of the reference, and optionally can merge together OTUs in both files (including the counts in the reads mapping table) if multiple OTUs match the same reference, and/or make adjustments if your references came from any specific samples.
    # Check the help
MBC_refmatcher.py --help

    # Just rename OTUs, don't merge multiple OTUs if they match the same referenc
MBC_refmatcher.py -f otus.fasta -u otus_search.uc -t reads_map.tsv -i 0.98 --dontmerge

    # Rename OTUs that match a reference and merge OTUs that match the same reference together
MBC_refmatcher.py -f otus.fasta -u otus_search.uc -t reads_map.tsv -i 0.98

    # If your reference specimens were drawn from your original sample, you might want to adjust your reads table
    # to approximate the increased reads you would have gotten for the equivalent OTU had you not removed that specimen
MBC_refmatcher.py -f otus.fasta -u otus_search.uc -t reads_map.tsv -i 0.98 --editreads
